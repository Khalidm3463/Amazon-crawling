{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdeca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43255fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract Product URL\n",
    "def get_url(soup):\n",
    "    try:\n",
    "        url = soup.find(\"a\", attrs={\"class\":\"a-link-normal a-text-normal\"})['href']\n",
    "    except AttributeError:\n",
    "        url = \"\"\n",
    "    return url\n",
    "\n",
    "# Function to extract Product Title\n",
    "def get_title(soup):\n",
    "    try:\n",
    "        title = soup.find(\"span\", attrs={\"class\":\"a-size-base-plus a-color-base a-text-normal\"}).text.strip()\n",
    "    except AttributeError:\n",
    "        title = \"\"\n",
    "    return title\n",
    "\n",
    "# Function to extract Product Price\n",
    "def get_price(soup):\n",
    "    try:\n",
    "        price = soup.find(\"span\", attrs={\"class\":\"a-price-whole\"}).text.strip()\n",
    "    except AttributeError:\n",
    "        price = \"\"\n",
    "    return price\n",
    "\n",
    "# Function to extract Product Rating\n",
    "def get_rating(soup):\n",
    "    try:\n",
    "        rating = soup.find(\"span\", attrs={\"class\":\"a-icon-alt\"}).text.strip()\n",
    "    except AttributeError:\n",
    "        rating = \"\"\n",
    "    return rating\n",
    "\n",
    "# Function to extract Number of User Reviews\n",
    "def get_review_count(soup):\n",
    "    try:\n",
    "        review_count = soup.find(\"span\", attrs={\"class\":\"a-size-base\"}).text.strip()\n",
    "    except AttributeError:\n",
    "        review_count = \"\"\n",
    "    return review_count\n",
    "\n",
    "# Function to extract Product Description\n",
    "def get_description(soup):\n",
    "    try:\n",
    "        description = soup.find(\"div\", attrs={\"id\":\"productDescription\"}).text.strip()\n",
    "    except AttributeError:\n",
    "        description = \"\"\n",
    "    return description\n",
    "\n",
    "# Function to extract ASIN\n",
    "def get_asin(soup):\n",
    "    try:\n",
    "        asin = soup.find(\"th\", text=\"ASIN\").find_next_sibling(\"td\").text.strip()\n",
    "    except AttributeError:\n",
    "        asin = \"\"\n",
    "    return asin\n",
    "\n",
    "# Function to extract Product Manufacturer\n",
    "def get_manufacturer(soup):\n",
    "    try:\n",
    "        manufacturer = soup.find(\"th\", text=\"Manufacturer\").find_next_sibling(\"td\").text.strip()\n",
    "    except AttributeError:\n",
    "        manufacturer = \"\"\n",
    "    return manufacturer\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Add your user agent\n",
    "    HEADERS = ({'User-Agent':'', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "    # Load the URLs from the CSV file\n",
    "    amazon_df = pd.read_csv(\"amazon_data.csv\")\n",
    "\n",
    "    d = {\"url\":[], \"title\":[], \"price\":[], \"rating\":[], \"reviews\":[], \"description\":[], \"asin\":[], \"manufacturer\":[]}\n",
    "\n",
    "    # Loop for scraping product details from each URL\n",
    "    for url in amazon_df[\"url\"]:\n",
    "        # HTTP Request\n",
    "        webpage = requests.get(url, headers=HEADERS)\n",
    "\n",
    "        # Soup Object containing all data\n",
    "        soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "\n",
    "        # Function calls to extract necessary product information\n",
    "        d['url'].append(url)\n",
    "        d['title'].append(get_title(soup))\n",
    "        d['price'].append(get_price(soup))\n",
    "        d['rating'].append(get_rating(soup))\n",
    "        d['reviews'].append(get_review_count(soup))\n",
    "        d['description'].append(get_description(soup))\n",
    "        d['asin'].append(get_asin(soup))\n",
    "        d['manufacturer'].append(get_manufacturer(soup))\n",
    "\n",
    "    # Create a dataframe from the dictionary\n",
    "    final_df = pd.DataFrame.from_dict(d)\n",
    "\n",
    "    # Drop any rows where title is not available\n",
    "    final_df['title'].replace('', np.nan, inplace=True)\n",
    "    final_df = final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a097a53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # Add your user agent\n",
    "    HEADERS = ({'User-Agent':'', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "    # Load the URLs from the CSV file\n",
    "    amazon_df = pd.read_csv(\"amazon_data.csv\")\n",
    "\n",
    "    d = {\"url\":[], \"title\":[], \"price\":[], \"rating\":[], \"reviews\":[], \"description\":[], \"asin\":[], \"manufacturer\":[]}\n",
    "\n",
    "    # Loop for scraping product details from each URL\n",
    "    for url in amazon_df[\"url\"]:\n",
    "        # HTTP Request\n",
    "        webpage = requests.get(url, headers=HEADERS)\n",
    "\n",
    "        # Soup Object containing all data\n",
    "        soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "\n",
    "        # Function calls to extract necessary product information\n",
    "        d['url'].append(url)\n",
    "        d['title'].append(get_title(soup))\n",
    "        d['price'].append(get_price(soup))\n",
    "        d['rating'].append(get_rating(soup))\n",
    "        d['reviews'].append(get_review_count(soup))\n",
    "        d['description'].append(get_description(soup))\n",
    "        d['asin'].append(get_asin(soup))\n",
    "        d['manufacturer'].append(get_manufacturer(soup))\n",
    "\n",
    "    # Create a dataframe from the dictionary\n",
    "    final_df = pd.DataFrame.from_dict(d)\n",
    "    # Drop any rows where title is not available\n",
    "    final_df['title'].replace('', np.nan, inplace=True)\n",
    "    final_df = final\n",
    "    \n",
    "    # HTTP Request\n",
    "    webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "    # Soup Object containing all data\n",
    "    soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "\n",
    "    # Fetch all products on the page as List of Tag Objects\n",
    "    products = soup.find_all(\"div\", attrs={'data-component-type':'s-search-result'})\n",
    "\n",
    "    # Loop for extracting product details from each product \n",
    "    for product in products:\n",
    "        # Function calls to extract necessary product information\n",
    "        url = \"https://www.amazon.in\" + get_url(product)\n",
    "        title = get_title(product)\n",
    "        price = get_price(product)\n",
    "        rating = get_rating(product)\n",
    "        reviews = get_review_count(product)\n",
    "\n",
    "        # HTTP Request to get the product page\n",
    "        product_page = requests.get(url, headers=HEADERS)\n",
    "\n",
    "        # Soup object containing product page data\n",
    "        product_soup = BeautifulSoup(product_page.content, \"html.parser\")\n",
    "\n",
    "        # Extracting ASIN\n",
    "        asin = product_soup.find(\"span\", attrs={\"class\":\"a-size-base a-text-bold\"}).text.strip()\n",
    "\n",
    "        # Extracting product description\n",
    "        description = product_soup.find(\"div\", attrs={\"id\":\"productDescription\"}).text.strip()\n",
    "\n",
    "        # Extracting manufacturer\n",
    "        manufacturer = product_soup.find(\"a\", attrs={\"id\":\"bylineInfo\"}).text.strip()\n",
    "\n",
    "        # Appending product details to list\n",
    "        product_details.append({\"url\":url, \"title\":title, \"price\":price, \"rating\":rating, \"reviews\":reviews, \n",
    "                                \"asin\":asin, \"description\":description, \"manufacturer\":manufacturer})\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbe5848",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_df = pd.DataFrame(product_details)\n",
    "\n",
    "#Drop any rows where title is not available\n",
    "amazon_df['title'].replace('', np.nan, inplace=True)\n",
    "amazon_df = amazon_df.dropna(subset=['title'])\n",
    "\n",
    "#Save the dataframe to a csv file\n",
    "amazon_df.to_csv(\"amazon_data.csv\", header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
